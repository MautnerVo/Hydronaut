{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:29.310163Z",
     "start_time": "2025-11-15T15:06:29.306836Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:29.318011Z",
     "start_time": "2025-11-15T15:06:29.315805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = r\"Y:\\Datasets\\Fyzio\"\n",
    "exercise = \"Wide squat\"\n"
   ],
   "id": "d72b3714ae6c6950",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.105229Z",
     "start_time": "2025-11-15T15:06:29.329093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(os.path.join(path,f\"X_Class_train_w_augmentation\",exercise + \".pkl\"),\"rb\") as f:\n",
    "    x = np.array(pickle.load(f))\n",
    "    print(x.shape)\n",
    "\n"
   ],
   "id": "c1f8f7fea4f2bdfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136, 200, 20)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.135393Z",
     "start_time": "2025-11-15T15:06:40.124730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"---- DATA DIAGNOSTICS ----\")\n",
    "print(\"Shape:\", x.shape)\n",
    "print(\"Dtype:\", x.dtype)\n",
    "print(\"NaN:\", np.isnan(x).any())\n",
    "print(\"Inf:\", np.isinf(x).any())"
   ],
   "id": "1a9fcf5048f9ac49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DATA DIAGNOSTICS ----\n",
      "Shape: (1136, 200, 20)\n",
      "Dtype: float64\n",
      "NaN: False\n",
      "Inf: False\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.369085Z",
     "start_time": "2025-11-15T15:06:40.142905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(os.path.join(path,\"Y_Class_train_w_augmentation\",exercise + \".pkl\"),\"rb\") as f:\n",
    "    y = np.array(pickle.load(f))\n",
    "    print(y.shape)\n"
   ],
   "id": "cba7e8ccf8dffd95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136, 3)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.510460Z",
     "start_time": "2025-11-15T15:06:40.376245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "x_norm = np.zeros_like(x)\n",
    "means = []\n",
    "stds = []\n",
    "print(x.shape)\n",
    "for ch in range(x.shape[2]):\n",
    "    mean = x[:, :, ch].mean()\n",
    "    std = x[:, :, ch].std()\n",
    "    x_norm[:, :, ch] = (x[:, :, ch] - mean) / std\n",
    "    means.append(mean)\n",
    "    stds.append(std)"
   ],
   "id": "241bff055e83baa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136, 200, 20)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.570282Z",
     "start_time": "2025-11-15T15:06:40.517302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "y_norm_t = torch.from_numpy(y).float()\n",
    "x_norm_t = torch.from_numpy(x_norm).float()\n",
    "x_norm_t = x_norm_t.permute(0,2,1)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_norm_t,y_norm_t,test_size=0.3,random_state=42)\n",
    "unique, counts = torch.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u.item()}: {c.item()} samples\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(x_train.shape)\n"
   ],
   "id": "bd6d1879efb5f841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0: 569 samples\n",
      "Class 1.0: 1816 samples\n",
      "cuda\n",
      "2.7.1+cu128\n",
      "12.8\n",
      "90701\n",
      "torch.Size([795, 20, 200])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.579938Z",
     "start_time": "2025-11-15T15:06:40.575441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class ClassCNN(nn.Module):\n",
    "    def __init__(self,in_channels=20,out_channels=3):\n",
    "        super(ClassCNN,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.normalization1=nn.BatchNorm1d(64)\n",
    "        self.pool1= nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.normalization2=nn.BatchNorm1d(128)\n",
    "        self.pool2= nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
    "        self.normalization3=nn.BatchNorm1d(64)\n",
    "        self.pool3= nn.MaxPool1d(2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64, 8)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(8, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.normalization1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.normalization2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.normalization3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n"
   ],
   "id": "b82fc247b2367103",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:06:40.591300Z",
     "start_time": "2025-11-15T15:06:40.586644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def add_noise(x, noise_factor=0.001):\n",
    "    return x + noise_factor * torch.randn_like(x)\n",
    "\n",
    "def time_shift(x, shift_max=10):\n",
    "    shift = int(torch.randint(-shift_max, shift_max+1, (1,)))\n",
    "    return torch.roll(x, shifts=shift)\n",
    "\n",
    "def scale(x, factor_range=(0.9, 1.1)):\n",
    "    factor = torch.empty(1).uniform_(*factor_range)\n",
    "    return x * factor\n",
    "\n",
    "def drop(x):\n",
    "    x = x.clone()\n",
    "    n_channels = x.shape[0]\n",
    "    ch = torch.randint(0, n_channels,(1,)).item()\n",
    "    x[ch, :] = 0.0\n",
    "    return x\n",
    "\n",
    "def random_augment(x):\n",
    "    if torch.rand(1) < 0.5:\n",
    "        x = add_noise(x)\n",
    "    if torch.rand(1) < 0.4:\n",
    "        x = time_shift(x)\n",
    "    if torch.rand(1) < 0.3:\n",
    "        x = scale(x)\n",
    "    if torch.rand(1) < 0.4:\n",
    "        x = drop(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class AugmentedSignalDataset(Dataset):\n",
    "    def __init__(self, X, Y, augment=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        if self.augment:\n",
    "            x = random_augment(x)\n",
    "        return x,y"
   ],
   "id": "55a375f396169dc8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T15:29:26.178839Z",
     "start_time": "2025-11-15T15:29:16.578868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = ClassCNN(in_channels=20, out_channels=3).to(device)\n",
    "\n",
    "pos_weights = torch.tensor([19/3, 19/8, 19/4], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "X_train_tensor = x_train.detach().clone()\n",
    "Y_train_tensor = y_train.detach().clone().float()\n",
    "X_val_tensor = x_test.detach().clone()\n",
    "Y_val_tensor = y_test.detach().clone().float()\n",
    "\n",
    "train_dataset = AugmentedSignalDataset(X_train_tensor, Y_train_tensor, augment=False)\n",
    "val_dataset = AugmentedSignalDataset(X_val_tensor, Y_val_tensor, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == batch_y).all(dim=1).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            val_outputs = net(batch_X)\n",
    "            loss = criterion(val_outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(val_outputs) > 0.5).float()\n",
    "            correct += (preds == batch_y).all(dim=1).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ],
   "id": "21e35b9bfbc0636f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 2.2024, Train Acc: 0.2931, Val Loss: 2.1405, Val Acc: 0.3812\n",
      "Epoch 2/100, Train Loss: 2.0791, Train Acc: 0.3447, Val Loss: 1.9519, Val Acc: 0.5015\n",
      "Epoch 3/100, Train Loss: 1.9846, Train Acc: 0.3887, Val Loss: 1.8561, Val Acc: 0.5103\n",
      "Epoch 4/100, Train Loss: 1.9434, Train Acc: 0.3899, Val Loss: 1.7860, Val Acc: 0.5367\n",
      "Epoch 5/100, Train Loss: 1.9107, Train Acc: 0.4390, Val Loss: 1.7367, Val Acc: 0.5543\n",
      "Epoch 6/100, Train Loss: 1.8206, Train Acc: 0.4491, Val Loss: 1.6812, Val Acc: 0.5630\n",
      "Epoch 7/100, Train Loss: 1.8173, Train Acc: 0.4453, Val Loss: 1.6406, Val Acc: 0.5630\n",
      "Epoch 8/100, Train Loss: 1.7708, Train Acc: 0.4553, Val Loss: 1.6142, Val Acc: 0.5660\n",
      "Epoch 9/100, Train Loss: 1.7686, Train Acc: 0.4226, Val Loss: 1.5602, Val Acc: 0.5748\n",
      "Epoch 10/100, Train Loss: 1.7288, Train Acc: 0.4403, Val Loss: 1.5371, Val Acc: 0.5836\n",
      "Epoch 11/100, Train Loss: 1.7055, Train Acc: 0.4503, Val Loss: 1.5093, Val Acc: 0.5748\n",
      "Epoch 12/100, Train Loss: 1.6875, Train Acc: 0.4717, Val Loss: 1.4867, Val Acc: 0.5894\n",
      "Epoch 13/100, Train Loss: 1.6626, Train Acc: 0.4629, Val Loss: 1.4519, Val Acc: 0.5748\n",
      "Epoch 14/100, Train Loss: 1.6636, Train Acc: 0.4692, Val Loss: 1.4272, Val Acc: 0.5806\n",
      "Epoch 15/100, Train Loss: 1.6533, Train Acc: 0.4667, Val Loss: 1.4023, Val Acc: 0.5806\n",
      "Epoch 16/100, Train Loss: 1.6182, Train Acc: 0.4868, Val Loss: 1.3555, Val Acc: 0.5894\n",
      "Epoch 17/100, Train Loss: 1.6220, Train Acc: 0.4893, Val Loss: 1.3514, Val Acc: 0.5924\n",
      "Epoch 18/100, Train Loss: 1.5672, Train Acc: 0.5082, Val Loss: 1.3140, Val Acc: 0.6041\n",
      "Epoch 19/100, Train Loss: 1.5713, Train Acc: 0.4943, Val Loss: 1.2763, Val Acc: 0.6100\n",
      "Epoch 20/100, Train Loss: 1.5724, Train Acc: 0.5044, Val Loss: 1.2600, Val Acc: 0.6070\n",
      "Epoch 21/100, Train Loss: 1.5468, Train Acc: 0.4767, Val Loss: 1.2558, Val Acc: 0.6100\n",
      "Epoch 22/100, Train Loss: 1.5015, Train Acc: 0.5082, Val Loss: 1.2316, Val Acc: 0.6070\n",
      "Epoch 23/100, Train Loss: 1.4992, Train Acc: 0.4918, Val Loss: 1.1981, Val Acc: 0.6070\n",
      "Epoch 24/100, Train Loss: 1.5240, Train Acc: 0.5044, Val Loss: 1.1931, Val Acc: 0.6070\n",
      "Epoch 25/100, Train Loss: 1.4923, Train Acc: 0.4931, Val Loss: 1.1812, Val Acc: 0.6100\n",
      "Epoch 26/100, Train Loss: 1.4512, Train Acc: 0.4943, Val Loss: 1.1527, Val Acc: 0.6070\n",
      "Epoch 27/100, Train Loss: 1.4842, Train Acc: 0.5195, Val Loss: 1.1354, Val Acc: 0.6100\n",
      "Epoch 28/100, Train Loss: 1.4606, Train Acc: 0.4881, Val Loss: 1.1217, Val Acc: 0.6070\n",
      "Epoch 29/100, Train Loss: 1.4364, Train Acc: 0.5057, Val Loss: 1.1101, Val Acc: 0.6070\n",
      "Epoch 30/100, Train Loss: 1.4545, Train Acc: 0.5208, Val Loss: 1.0823, Val Acc: 0.6070\n",
      "Epoch 31/100, Train Loss: 1.4369, Train Acc: 0.5044, Val Loss: 1.0652, Val Acc: 0.6070\n",
      "Epoch 32/100, Train Loss: 1.4292, Train Acc: 0.5019, Val Loss: 1.0748, Val Acc: 0.6100\n",
      "Epoch 33/100, Train Loss: 1.3857, Train Acc: 0.5245, Val Loss: 1.0457, Val Acc: 0.6070\n",
      "Epoch 34/100, Train Loss: 1.3784, Train Acc: 0.5157, Val Loss: 1.0355, Val Acc: 0.6070\n",
      "Epoch 35/100, Train Loss: 1.3862, Train Acc: 0.5447, Val Loss: 1.0326, Val Acc: 0.6100\n",
      "Epoch 36/100, Train Loss: 1.3944, Train Acc: 0.5082, Val Loss: 1.0085, Val Acc: 0.6129\n",
      "Epoch 37/100, Train Loss: 1.3654, Train Acc: 0.5711, Val Loss: 0.9992, Val Acc: 0.6100\n",
      "Epoch 38/100, Train Loss: 1.3338, Train Acc: 0.5233, Val Loss: 0.9845, Val Acc: 0.6129\n",
      "Epoch 39/100, Train Loss: 1.3624, Train Acc: 0.5522, Val Loss: 0.9799, Val Acc: 0.6100\n",
      "Epoch 40/100, Train Loss: 1.3555, Train Acc: 0.5572, Val Loss: 0.9676, Val Acc: 0.6188\n",
      "Epoch 41/100, Train Loss: 1.3026, Train Acc: 0.5660, Val Loss: 0.9641, Val Acc: 0.6246\n",
      "Epoch 42/100, Train Loss: 1.3604, Train Acc: 0.5447, Val Loss: 0.9479, Val Acc: 0.6246\n",
      "Epoch 43/100, Train Loss: 1.3032, Train Acc: 0.5748, Val Loss: 0.9392, Val Acc: 0.6188\n",
      "Epoch 44/100, Train Loss: 1.3152, Train Acc: 0.5535, Val Loss: 0.9255, Val Acc: 0.6334\n",
      "Epoch 45/100, Train Loss: 1.2465, Train Acc: 0.5723, Val Loss: 0.9151, Val Acc: 0.6628\n",
      "Epoch 46/100, Train Loss: 1.2824, Train Acc: 0.5635, Val Loss: 0.9036, Val Acc: 0.6334\n",
      "Epoch 47/100, Train Loss: 1.2941, Train Acc: 0.5635, Val Loss: 0.9015, Val Acc: 0.6364\n",
      "Epoch 48/100, Train Loss: 1.2783, Train Acc: 0.5673, Val Loss: 0.8748, Val Acc: 0.6364\n",
      "Epoch 49/100, Train Loss: 1.2990, Train Acc: 0.5509, Val Loss: 0.8665, Val Acc: 0.6452\n",
      "Epoch 50/100, Train Loss: 1.2540, Train Acc: 0.5711, Val Loss: 0.8551, Val Acc: 0.6657\n",
      "Epoch 51/100, Train Loss: 1.2454, Train Acc: 0.5912, Val Loss: 0.8459, Val Acc: 0.6921\n",
      "Epoch 52/100, Train Loss: 1.2473, Train Acc: 0.5723, Val Loss: 0.8432, Val Acc: 0.7097\n",
      "Epoch 53/100, Train Loss: 1.2114, Train Acc: 0.5811, Val Loss: 0.8115, Val Acc: 0.7038\n",
      "Epoch 54/100, Train Loss: 1.2171, Train Acc: 0.5686, Val Loss: 0.8152, Val Acc: 0.7419\n",
      "Epoch 55/100, Train Loss: 1.2127, Train Acc: 0.5648, Val Loss: 0.8064, Val Acc: 0.7507\n",
      "Epoch 56/100, Train Loss: 1.1945, Train Acc: 0.5748, Val Loss: 0.7838, Val Acc: 0.7537\n",
      "Epoch 57/100, Train Loss: 1.1864, Train Acc: 0.5572, Val Loss: 0.7648, Val Acc: 0.7331\n",
      "Epoch 58/100, Train Loss: 1.1651, Train Acc: 0.5711, Val Loss: 0.7594, Val Acc: 0.7507\n",
      "Epoch 59/100, Train Loss: 1.1725, Train Acc: 0.5535, Val Loss: 0.7434, Val Acc: 0.7713\n",
      "Epoch 60/100, Train Loss: 1.1948, Train Acc: 0.5862, Val Loss: 0.7387, Val Acc: 0.7566\n",
      "Epoch 61/100, Train Loss: 1.1976, Train Acc: 0.5723, Val Loss: 0.7306, Val Acc: 0.7155\n",
      "Epoch 62/100, Train Loss: 1.1356, Train Acc: 0.5774, Val Loss: 0.7284, Val Acc: 0.7243\n",
      "Epoch 63/100, Train Loss: 1.1667, Train Acc: 0.5648, Val Loss: 0.7183, Val Acc: 0.7507\n",
      "Epoch 64/100, Train Loss: 1.1429, Train Acc: 0.5686, Val Loss: 0.7111, Val Acc: 0.7390\n",
      "Epoch 65/100, Train Loss: 1.0993, Train Acc: 0.5736, Val Loss: 0.7097, Val Acc: 0.7683\n",
      "Epoch 66/100, Train Loss: 1.1507, Train Acc: 0.5836, Val Loss: 0.6856, Val Acc: 0.7859\n",
      "Epoch 67/100, Train Loss: 1.0932, Train Acc: 0.5698, Val Loss: 0.6880, Val Acc: 0.7889\n",
      "Epoch 68/100, Train Loss: 1.0999, Train Acc: 0.5899, Val Loss: 0.6652, Val Acc: 0.7830\n",
      "Epoch 69/100, Train Loss: 1.1080, Train Acc: 0.5849, Val Loss: 0.6724, Val Acc: 0.7889\n",
      "Epoch 70/100, Train Loss: 1.1046, Train Acc: 0.5673, Val Loss: 0.6619, Val Acc: 0.7947\n",
      "Epoch 71/100, Train Loss: 1.0842, Train Acc: 0.5572, Val Loss: 0.6542, Val Acc: 0.7947\n",
      "Epoch 72/100, Train Loss: 1.0935, Train Acc: 0.5597, Val Loss: 0.6425, Val Acc: 0.7918\n",
      "Epoch 73/100, Train Loss: 1.0808, Train Acc: 0.5585, Val Loss: 0.6474, Val Acc: 0.7830\n",
      "Epoch 74/100, Train Loss: 1.1285, Train Acc: 0.5484, Val Loss: 0.6307, Val Acc: 0.7947\n",
      "Epoch 75/100, Train Loss: 1.0237, Train Acc: 0.5824, Val Loss: 0.6218, Val Acc: 0.7859\n",
      "Epoch 76/100, Train Loss: 1.0768, Train Acc: 0.5509, Val Loss: 0.6121, Val Acc: 0.8006\n",
      "Epoch 77/100, Train Loss: 1.0609, Train Acc: 0.5736, Val Loss: 0.6042, Val Acc: 0.7859\n",
      "Epoch 78/100, Train Loss: 1.0584, Train Acc: 0.5585, Val Loss: 0.6066, Val Acc: 0.7977\n",
      "Epoch 79/100, Train Loss: 1.0520, Train Acc: 0.5862, Val Loss: 0.5991, Val Acc: 0.8006\n",
      "Epoch 80/100, Train Loss: 1.0199, Train Acc: 0.5887, Val Loss: 0.5888, Val Acc: 0.7947\n",
      "Epoch 81/100, Train Loss: 1.0423, Train Acc: 0.5547, Val Loss: 0.5803, Val Acc: 0.7947\n",
      "Epoch 82/100, Train Loss: 1.0596, Train Acc: 0.5535, Val Loss: 0.5714, Val Acc: 0.7977\n",
      "Epoch 83/100, Train Loss: 1.0459, Train Acc: 0.5887, Val Loss: 0.5714, Val Acc: 0.7977\n",
      "Epoch 84/100, Train Loss: 1.0648, Train Acc: 0.5660, Val Loss: 0.5714, Val Acc: 0.7977\n",
      "Epoch 85/100, Train Loss: 1.0421, Train Acc: 0.5686, Val Loss: 0.5571, Val Acc: 0.7889\n",
      "Epoch 86/100, Train Loss: 1.0193, Train Acc: 0.5585, Val Loss: 0.5561, Val Acc: 0.7947\n",
      "Epoch 87/100, Train Loss: 1.0215, Train Acc: 0.5635, Val Loss: 0.5541, Val Acc: 0.8035\n",
      "Epoch 88/100, Train Loss: 0.9821, Train Acc: 0.5899, Val Loss: 0.5537, Val Acc: 0.8065\n",
      "Epoch 89/100, Train Loss: 1.0203, Train Acc: 0.5535, Val Loss: 0.5445, Val Acc: 0.8065\n",
      "Epoch 90/100, Train Loss: 1.0475, Train Acc: 0.5648, Val Loss: 0.5329, Val Acc: 0.8065\n",
      "Epoch 91/100, Train Loss: 1.0589, Train Acc: 0.5522, Val Loss: 0.5318, Val Acc: 0.8006\n",
      "Epoch 92/100, Train Loss: 0.9892, Train Acc: 0.5648, Val Loss: 0.5258, Val Acc: 0.8035\n",
      "Epoch 93/100, Train Loss: 1.0274, Train Acc: 0.5623, Val Loss: 0.5191, Val Acc: 0.8065\n",
      "Epoch 94/100, Train Loss: 0.9703, Train Acc: 0.5648, Val Loss: 0.5173, Val Acc: 0.8065\n",
      "Epoch 95/100, Train Loss: 0.9792, Train Acc: 0.5660, Val Loss: 0.5101, Val Acc: 0.7889\n",
      "Epoch 96/100, Train Loss: 0.9937, Train Acc: 0.5887, Val Loss: 0.4974, Val Acc: 0.8065\n",
      "Epoch 97/100, Train Loss: 0.9745, Train Acc: 0.5824, Val Loss: 0.4868, Val Acc: 0.8123\n",
      "Epoch 98/100, Train Loss: 0.9454, Train Acc: 0.5849, Val Loss: 0.4792, Val Acc: 0.8475\n",
      "Epoch 99/100, Train Loss: 0.9000, Train Acc: 0.6151, Val Loss: 0.4724, Val Acc: 0.8563\n",
      "Epoch 100/100, Train Loss: 0.9213, Train Acc: 0.5862, Val Loss: 0.4584, Val Acc: 0.8592\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
